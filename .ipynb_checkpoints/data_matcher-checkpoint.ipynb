{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "098b341e-b695-4170-b1d8-7052589ea390",
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "808a9cf1-73a8-4eb2-bbe2-0d35b38b5984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base directory\n",
    "sat_dir = os.path.expanduser('~/sea-level-seekers/iharp_training_dataset/Copernicus_ENA_Satelite_Maps_Training_Data')\n",
    "sat_names = os.listdir(sat_dir)\n",
    "flood_dir = os.path.expanduser('~/sea-level-seekers/iharp_training_dataset/Flooding_Data')\n",
    "flood_names = os.listdir(flood_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b40f80b-bccd-4f62-9173-e02d7e26086f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all file names with their full paths\n",
    "try:\n",
    "    sat_names = [os.path.join(sat_dir, f) for f in os.listdir(sat_dir)]\n",
    "    flood_names = [os.path.join(flood_dir, f) for f in os.listdir(flood_dir) if f.endswith('.csv')]\n",
    "except FileNotFoundError:\n",
    "    print(f\"Directory not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "908711ba-6678-4d79-8b2a-6b9bf0940fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_extractor(filename):\n",
    "    file_name = filename\n",
    "    \n",
    "    # Regular expression to extract the date\n",
    "    match = re.search(r'dt_ena_(\\d+)_vDT', file_name)\n",
    "    date_str = match.group(1)  # '19930101'\n",
    "    \n",
    "    # Format the date as 'YYYY-MM-DD'\n",
    "    formatted_date = f\"{date_str[:4]}-{date_str[4:6]}-{date_str[6:]}\"\n",
    "    \n",
    "    return formatted_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9d515c2a-bf65-4c28-99b9-828433897a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_dates = [date_extractor(sat_name) for sat_name in sat_names]\n",
    "sat_dates_set = set(sat_dates)\n",
    "\n",
    "date_range = pd.date_range(start='1993-01-01', end='2013-12-31').date\n",
    "date_range_str = [date.strftime('%Y-%m-%d') for date in date_range]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3e0215b9-1313-4788-aa97-be0d2365de4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_dates = [date for date in date_range_str if date not in sat_dates_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe49cf8-b923-4db2-872d-1606ea344249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# for file in flood_names:\n",
    "#     # Read the CSV file\n",
    "#     df = pd.read_csv(file)\n",
    "#     location = df['location'][0]\n",
    "#     modified_location = location.lower().replace(\" \", \"_\")\n",
    "#     anomaly = 0\n",
    "#     latitude = df['latitude'][0]\n",
    "#     longitude = df['longitude'][0]\n",
    "    \n",
    "#     # Convert 't' (date) column to datetime format\n",
    "#     df['t'] = pd.to_datetime(df['t'])\n",
    "    \n",
    "#     # Set the date range from 1993-01-01 to 2013-12-31\n",
    "#     date_range = pd.date_range(start='1993-01-01', end='2013-12-31')\n",
    "    \n",
    "#     # Ensure there is only one row per day\n",
    "#     df = df.drop_duplicates(subset=['t'])\n",
    "    \n",
    "#     # Reindex the dataframe with the complete date range\n",
    "#     df = df.set_index('t').reindex(date_range, fill_value=np.nan)\n",
    "    \n",
    "#     # Fill missing values for 'anomaly', 'location', 'latitude', and 'longitude'\n",
    "#     df['anomaly'].fillna(anomaly, inplace=True)\n",
    "#     df['location'].fillna(location, inplace=True)\n",
    "#     df['latitude'].fillna(latitude, inplace=True)\n",
    "#     df['longitude'].fillna(longitude, inplace=True)\n",
    "    \n",
    "#     # Reset the index to get the date column back\n",
    "#     df.reset_index(inplace=True)\n",
    "#     df.rename(columns={'index': 't'}, inplace=True)\n",
    "    \n",
    "#     # Save the cleaned data to a new CSV file\n",
    "#     df.to_csv('/home/jovyan/sea-level-seekers/iharp_training_dataset/cleaned_flooding_' + modified_location, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "580ae306-f15c-4107-81b3-91ccb8299d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "df = pd.read_csv(flood_names[0])\n",
    "location = df['location'][0]\n",
    "modified_location = location.lower().replace(\" \", \"_\")\n",
    "anomaly = int(0)\n",
    "latitude = df['latitude'][0]\n",
    "longitude = df['longitude'][0]\n",
    "\n",
    "# Convert 't' (date) column to datetime format\n",
    "df['t'] = pd.to_datetime(df['t'])\n",
    "\n",
    "# Set the date range from 1993-01-01 to 2013-12-31\n",
    "date_range = pd.date_range(start='1993-01-01', end='2013-12-31')\n",
    "\n",
    "# Ensure there is only one row per day\n",
    "df = df.drop_duplicates(subset=['t'])\n",
    "\n",
    "# Reindex the dataframe with the complete date range\n",
    "df = df.set_index('t').reindex(date_range, fill_value=np.nan)\n",
    "\n",
    "# Fill missing values for 'anomaly', 'location', 'latitude', and 'longitude'\n",
    "df['anomaly'].fillna(anomaly)\n",
    "df['location'].fillna(location)\n",
    "df['latitude'].fillna(latitude)\n",
    "df['longitude'].fillna(longitude)\n",
    "\n",
    "# Reset the index to get the date column back\n",
    "df.reset_index(inplace=True)\n",
    "df.rename(columns={'index': 't'}, inplace=True)\n",
    "\n",
    "missing_dates = pd.to_datetime(missing_dates)\n",
    "df = df[~df['t'].isin(missing_dates)]\n",
    "\n",
    "# Save the cleaned data to a new CSV file\n",
    "df.to_csv('/home/jovyan/sea-level-seekers/iharp_training_dataset/cleaned_flooding/' + modified_location, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9877345e-e1fc-48c2-bbf5-b237ab6263ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e02b8a9-0302-4639-abd2-88e221fa8f2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
